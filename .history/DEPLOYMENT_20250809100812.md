# On-Premise Data Platform Deployment Guide

## Overview

This guide provides comprehensive instructions for deploying the on-premise data platform with Microsoft Fabric semantics integration. The platform runs entirely on-premise with only the semantic layer leveraging Microsoft Fabric for advanced analytics.

**Important Note**: This deployment involves collaboration between the Data Platform team and the Infrastructure team. The Infrastructure team manages the underlying infrastructure (servers, network, storage) while the Data Platform team provides requirements and manages the application layer.

## Team Collaboration Model

### Infrastructure Team Responsibilities
- **Server Management**: Provisioning and maintaining servers
- **Network Configuration**: Network setup, firewall rules, load balancers
- **Storage Management**: Storage allocation and backup infrastructure
- **Security Infrastructure**: Network security, SSL certificates, VPN access
- **Resource Monitoring**: Infrastructure monitoring and alerting
- **Disaster Recovery**: Backup and recovery infrastructure

### Data Platform Team Responsibilities
- **Application Architecture**: Design and maintain application architecture
- **Docker Configuration**: Manage docker-compose files and container orchestration
- **Application Deployment**: Deploy and configure applications
- **Data Management**: Database setup, data migration, data quality
- **Integration**: Microsoft Fabric integration and data synchronization
- **Application Monitoring**: Application-level monitoring and alerting

### Collaboration Points
- **Resource Requirements**: Data Platform team provides resource specifications
- **Network Requirements**: Data Platform team specifies network connectivity needs
- **Security Requirements**: Data Platform team defines security requirements
- **Deployment Coordination**: Joint planning for deployment schedules
- **Troubleshooting**: Collaborative problem-solving for infrastructure issues

## Architecture Overview

### On-Premise Components
- **Data Storage**: MySQL, InfluxDB, MinIO
- **ETL Processing**: Windmill (Primary), Apache Airflow (Alternative)
- **Monitoring**: Prometheus, Grafana, Open Metadata
- **Security**: Kong API Gateway, Azure AD integration
- **Visualization**: Grafana, Streamlit, Custom dashboards

### Microsoft Fabric Integration
- **Semantic Layer**: Data models, calculated fields, relationships
- **Power BI**: Interactive dashboards with semantic models
- **On-Premises Data Gateway**: Secure connectivity

## Prerequisites

### Infrastructure Requirements (Infrastructure Team)
- **OS**: Linux (Ubuntu 20.04+ recommended) or Windows Server 2019+
- **CPU**: 8+ cores
- **RAM**: 32GB+ 
- **Storage**: 500GB+ SSD
- **Network**: Stable internet connection for Microsoft Fabric integration
- **Network Security**: Firewall rules, VPN access, SSL certificates

### Software Requirements (Data Platform Team)
- Docker Engine 20.10+
- Docker Compose 2.0+
- Git
- Python 3.9+
- Azure CLI (for Microsoft Fabric integration)

### Microsoft Fabric Setup (Data Platform Team)
- Microsoft 365 Business or Enterprise subscription
- Power BI Pro or Premium license
- On-Premises Data Gateway installation

## Resource Requirements Specification

### Infrastructure Requirements for Infrastructure Team

#### Server Specifications
```yaml
# Production Environment
servers:
  data_platform_main:
    cpu: 8 cores
    ram: 32GB
    storage: 500GB SSD
    network: 1Gbps
    
  data_platform_backup:
    cpu: 4 cores
    ram: 16GB
    storage: 1TB HDD
    network: 1Gbps

# Development Environment
servers:
  data_platform_dev:
    cpu: 4 cores
    ram: 16GB
    storage: 250GB SSD
    network: 1Gbps
```

#### Network Requirements
```yaml
# Network Configuration
network:
  internal_subnet: 192.168.1.0/24
  external_access: true
  vpn_required: true
  
# Port Requirements
ports:
  - 3306: MySQL
  - 8086: InfluxDB
  - 9000: MinIO
  - 8000: Windmill
  - 3000: Grafana
  - 8585: Open Metadata
  - 8000: Kong API Gateway
  - 8501: Streamlit
```

#### Storage Requirements
```yaml
# Storage Configuration
storage:
  database_storage: 200GB
  object_storage: 500GB
  backup_storage: 1TB
  log_storage: 100GB
```

## Installation Steps

### 1. Infrastructure Setup (Infrastructure Team)

#### Server Provisioning
```bash
# Infrastructure team to provision servers with:
# - Operating system installation
# - Network configuration
# - Storage allocation
# - Security hardening
# - Monitoring setup
```

#### Network Configuration
```bash
# Infrastructure team to configure:
# - Firewall rules
# - Load balancers
# - VPN access
# - SSL certificates
# - DNS configuration
```

### 2. Environment Setup (Data Platform Team)

```bash
# Clone the repository
git clone <repository-url>
cd End-to-end-de

# Create environment file
cp config.env.example config.env
```

### 3. Configuration (Data Platform Team)

Edit `config.env` with your specific values:

```bash
# Database Configuration
MYSQL_ROOT_PASSWORD=your_secure_password
MYSQL_USER=data_user
MYSQL_PASSWORD=your_secure_password

# MinIO Configuration
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=your_secure_password

# InfluxDB Configuration
INFLUXDB_USERNAME=admin
INFLUXDB_PASSWORD=your_secure_password

# Monitoring Configuration
GRAFANA_PASSWORD=your_secure_password

# Airflow Configuration (if using alternative)
AIRFLOW_FERNET_KEY=your_fernet_key
```

### 4. Microsoft Fabric Integration Setup (Data Platform Team)

#### Install On-Premises Data Gateway

```bash
# Download and install On-Premises Data Gateway
# Follow Microsoft's official installation guide
# https://docs.microsoft.com/en-us/data-integration/gateway/service-gateway-install
```

#### Configure Data Gateway

1. **Register Gateway**:
   - Open Power BI Service
   - Go to Settings > Manage gateways
   - Add new gateway
   - Follow registration process

2. **Configure Data Sources**:
   - Add MySQL connection
   - Add InfluxDB connection
   - Configure authentication

3. **Set Up Data Refresh**:
   - Configure scheduled data refresh
   - Set up incremental refresh policies

### 5. Platform Deployment (Collaborative)

#### Start Core Services (Data Platform Team)

```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f
```

#### Infrastructure Verification (Infrastructure Team)

```bash
# Verify network connectivity
ping <server-ip>
telnet <server-ip> <port>

# Verify storage allocation
df -h
lsblk

# Verify resource usage
htop
free -h
```

### 6. Service Verification (Data Platform Team)

#### Check Service Health

```bash
# Database connectivity
docker exec data-platform-mysql mysql -u root -p -e "SELECT 1;"

# MinIO console
# Access: http://localhost:9001
# Username: minioadmin
# Password: your_configured_password

# Windmill ETL
# Access: http://localhost:8000

# Grafana monitoring
# Access: http://localhost:3000
# Username: admin
# Password: your_configured_password

# Kong API Gateway
# Access: http://localhost:8001 (Admin)
# Access: http://localhost:8000 (Proxy)
```

## Microsoft Fabric Integration

### 1. Semantic Model Creation (Data Platform Team)

#### Connect to On-Premise Data

1. **Open Power BI Desktop**
2. **Get Data** > **Database** > **MySQL database**
3. **Server**: Your on-premise MySQL server
4. **Database**: data_platform
5. **Authentication**: Use gateway credentials

#### Create Semantic Models

```sql
-- Example semantic model for IoT data
CREATE VIEW semantic_iot_summary AS
SELECT 
    device_id,
    device_type,
    DATE(timestamp) as date,
    AVG(temperature) as avg_temperature,
    MAX(temperature) as max_temperature,
    MIN(temperature) as min_temperature,
    COUNT(*) as reading_count
FROM iot_readings
GROUP BY device_id, device_type, DATE(timestamp);
```

### 2. Power BI Workspace Setup (Data Platform Team)

1. **Create Workspace**:
   - Open Power BI Service
   - Create new workspace
   - Set up workspace permissions

2. **Publish Semantic Models**:
   - Publish from Power BI Desktop
   - Configure refresh schedule
   - Set up row-level security

3. **Create Dashboards**:
   - Build interactive dashboards
   - Configure real-time data
   - Set up alerts and notifications

### 3. Data Synchronization (Data Platform Team)

#### Configure Incremental Refresh

```json
{
  "refreshPolicy": {
    "type": "incremental",
    "incrementalRefreshPolicy": {
      "refreshType": "full",
      "incrementalGranularity": "day",
      "incrementalWindow": 30
    }
  }
}
```

#### Set Up Scheduled Refresh

1. **Power BI Service**:
   - Go to dataset settings
   - Configure refresh schedule
   - Set up gateway data sources

2. **On-Premises Data Gateway**:
   - Configure refresh schedules
   - Set up error notifications
   - Monitor refresh status

## Security Configuration

### 1. Network Security (Infrastructure Team)

```bash
# Configure firewall rules
sudo ufw allow 3306/tcp  # MySQL
sudo ufw allow 8086/tcp  # InfluxDB
sudo ufw allow 9000/tcp  # MinIO
sudo ufw allow 8000/tcp  # Windmill
sudo ufw allow 3000/tcp  # Grafana
sudo ufw allow 8585/tcp  # Open Metadata
```

### 2. Azure AD Integration (Data Platform Team)

#### Configure Azure AD App Registration

1. **Create App Registration**:
   - Azure Portal > App registrations
   - Create new registration
   - Configure redirect URIs

2. **Set Up Authentication**:
   - Configure supported account types
   - Set up client secrets
   - Configure API permissions

3. **Integrate with Applications**:
   - Update application configurations
   - Test authentication flow

### 3. API Security (Collaborative)

#### Kong API Gateway Configuration

```yaml
# Example Kong configuration
services:
  - name: mysql-api
    url: http://mysql:3306
    routes:
      - name: mysql-route
        paths:
          - /api/mysql
    plugins:
      - name: key-auth
      - name: rate-limiting
        config:
          minute: 100
```

## Monitoring and Maintenance

### 1. Health Monitoring (Collaborative)

#### Prometheus Configuration (Data Platform Team)

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql:3306']
  
  - job_name: 'influxdb'
    static_configs:
      - targets: ['influxdb:8086']
  
  - job_name: 'windmill'
    static_configs:
      - targets: ['windmill:8000']
```

#### Infrastructure Monitoring (Infrastructure Team)

```bash
# Monitor server resources
htop
iotop
nethogs

# Monitor network connectivity
ping -c 5 <target-server>
traceroute <target-server>
```

#### Grafana Dashboards (Data Platform Team)

1. **Import Dashboards**:
   - MySQL monitoring dashboard
   - InfluxDB time-series dashboard
   - Windmill ETL monitoring
   - System resource monitoring

2. **Set Up Alerts**:
   - Database connection alerts
   - ETL job failure alerts
   - System resource alerts

### 2. Backup Strategy (Collaborative)

#### Database Backups (Data Platform Team)

```bash
# MySQL backup script
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
docker exec data-platform-mysql mysqldump -u root -p$MYSQL_ROOT_PASSWORD data_platform > backup_$DATE.sql

# InfluxDB backup
docker exec data-platform-influxdb influx backup /backup/backup_$DATE

# MinIO backup
mc mirror minio/data /backup/minio_$DATE
```

#### Infrastructure Backups (Infrastructure Team)

```bash
# Server backup
rsync -avz /data /backup/server_backup_$(date +%Y%m%d)

# Configuration backup
tar -czf /backup/config_backup_$(date +%Y%m%d).tar.gz /etc
```

#### Automated Backup Schedule (Collaborative)

```bash
# Add to crontab (Data Platform Team)
0 2 * * * /path/to/backup-script.sh

# Infrastructure team to ensure backup storage availability
```

### 3. Performance Optimization (Collaborative)

#### Database Optimization (Data Platform Team)

```sql
-- MySQL optimization
OPTIMIZE TABLE iot_readings;
ANALYZE TABLE device_metadata;

-- Create indexes for common queries
CREATE INDEX idx_timestamp ON iot_readings(timestamp);
CREATE INDEX idx_device_id ON iot_readings(device_id);
```

#### Infrastructure Optimization (Infrastructure Team)

```bash
# Monitor and optimize server performance
# - CPU usage optimization
# - Memory allocation
# - Storage I/O optimization
# - Network bandwidth optimization
```

#### Windmill ETL Optimization (Data Platform Team)

```python
# Example optimized Windmill script
import pandas as pd
from sqlalchemy import create_engine

def process_iot_data():
    # Use chunked processing for large datasets
    engine = create_engine('mysql://user:pass@mysql:3306/data_platform')
    
    for chunk in pd.read_sql('SELECT * FROM iot_readings', engine, chunksize=10000):
        # Process chunk
        processed_chunk = chunk.groupby('device_id').agg({
            'temperature': ['mean', 'max', 'min'],
            'humidity': ['mean', 'max', 'min']
        })
        
        # Save processed data
        processed_chunk.to_sql('processed_iot_data', engine, if_exists='append')
```

## Troubleshooting

### Common Issues

#### 1. Database Connection Issues (Data Platform Team)

```bash
# Check MySQL connectivity
docker exec data-platform-mysql mysql -u root -p -e "SHOW PROCESSLIST;"

# Check InfluxDB connectivity
curl -G http://localhost:8086/query --data-urlencode "q=SHOW DATABASES"
```

#### 2. Windmill ETL Issues (Data Platform Team)

```bash
# Check Windmill logs
docker-compose logs windmill

# Check Windmill database
docker exec data-platform-windmill-db psql -U windmill -d windmill -c "SELECT * FROM jobs;"
```

#### 3. Network Issues (Infrastructure Team)

```bash
# Check network connectivity
ping <target-server>
traceroute <target-server>
telnet <target-server> <port>

# Check firewall rules
sudo ufw status
sudo iptables -L
```

#### 4. Microsoft Fabric Integration Issues (Data Platform Team)

```bash
# Check On-Premises Data Gateway
# Windows: Check Windows Services
# Linux: Check systemd services

# Test gateway connectivity
# Use Power BI Gateway Configuration tool
```

### Performance Issues

#### 1. Slow Query Performance (Data Platform Team)

```sql
-- Check slow queries
SHOW VARIABLES LIKE 'slow_query_log';
SHOW VARIABLES LIKE 'long_query_time';

-- Analyze query performance
EXPLAIN SELECT * FROM iot_readings WHERE device_id = 'device_001';
```

#### 2. ETL Performance (Data Platform Team)

```python
# Monitor Windmill performance
import time

def monitor_etl_performance():
    start_time = time.time()
    # ETL process
    end_time = time.time()
    print(f"ETL took {end_time - start_time} seconds")
```

#### 3. Infrastructure Performance (Infrastructure Team)

```bash
# Monitor server performance
htop
iotop
nethogs

# Check resource usage
free -h
df -h
```

## Scaling Considerations

### 1. Horizontal Scaling (Collaborative)

#### Database Scaling (Infrastructure Team)

```yaml
# MySQL read replicas
services:
  mysql-replica:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    command: --read-only
```

#### Windmill Scaling (Data Platform Team)

```yaml
# Multiple Windmill instances
services:
  windmill-1:
    image: windmilllabs/windmill:latest
    environment:
      BASE_URL: http://localhost:8000
    ports:
      - "8000:8000"
  
  windmill-2:
    image: windmilllabs/windmill:latest
    environment:
      BASE_URL: http://localhost:8001
    ports:
      - "8001:8000"
```

### 2. Load Balancing (Infrastructure Team)

#### Kong Load Balancer Configuration

```yaml
# Kong load balancer
services:
  - name: windmill-cluster
    url: http://windmill-1:8000
    routes:
      - name: windmill-route
        paths:
          - /api/windmill
    plugins:
      - name: upstream
        config:
          algorithm: round-robin
          targets:
            - target: windmill-1:8000
              weight: 50
            - target: windmill-2:8000
              weight: 50
```

## Communication and Coordination

### 1. Regular Meetings

#### Weekly Status Meetings
- **Participants**: Data Platform team, Infrastructure team
- **Agenda**: 
  - Current status and issues
  - Upcoming changes
  - Resource requirements
  - Performance metrics

#### Monthly Planning Meetings
- **Participants**: Data Platform team, Infrastructure team, Stakeholders
- **Agenda**:
  - Capacity planning
  - Technology roadmap
  - Budget considerations
  - Risk assessment

### 2. Documentation and Knowledge Sharing

#### Shared Documentation
- **Architecture Documentation**: Both teams contribute
- **Runbooks**: Joint creation and maintenance
- **Troubleshooting Guides**: Collaborative development
- **Change Management**: Coordinated changes

#### Communication Channels
- **Slack/Teams**: Daily communication
- **Email**: Formal communications
- **Ticketing System**: Issue tracking and resolution
- **Wiki/Confluence**: Documentation repository

### 3. Escalation Procedures

#### Issue Escalation
1. **Level 1**: Data Platform team handles application issues
2. **Level 2**: Infrastructure team handles infrastructure issues
3. **Level 3**: Joint troubleshooting for complex issues
4. **Level 4**: Management escalation for critical issues

#### Change Management
1. **Change Request**: Submit change request with details
2. **Impact Assessment**: Both teams assess impact
3. **Approval Process**: Get necessary approvals
4. **Implementation**: Coordinated implementation
5. **Post-Implementation**: Review and documentation

## Conclusion

This on-premise deployment provides a robust, scalable data platform with Microsoft Fabric integration for advanced analytics. The hybrid approach ensures data sovereignty while leveraging cloud-based semantic modeling capabilities.

### Key Benefits
- **Data Sovereignty**: Complete control over data location
- **Security**: Enhanced security controls
- **Cost Control**: Predictable operational costs
- **Performance**: Optimized for local workloads
- **Compliance**: Easier regulatory compliance

### Team Collaboration Benefits
- **Clear Responsibilities**: Well-defined roles and responsibilities
- **Efficient Communication**: Regular meetings and communication channels
- **Shared Knowledge**: Collaborative documentation and knowledge sharing
- **Coordinated Changes**: Joint planning and implementation
- **Escalation Procedures**: Clear escalation paths for issues

### Next Steps
1. **Production Deployment**: Follow security hardening guidelines
2. **Monitoring Setup**: Configure comprehensive monitoring
3. **Backup Strategy**: Implement automated backup procedures
4. **Performance Tuning**: Optimize based on workload patterns
5. **Team Training**: Provide training on platform usage
6. **Process Documentation**: Document collaboration processes
